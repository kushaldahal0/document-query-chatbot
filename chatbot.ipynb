{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2Pq8jKDPqrjw0LqnugOAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushaldahal0/document-query-chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured[pdf]"
      ],
      "metadata": {
        "id": "zgYAKh105bM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "empWAO7AnN_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d754e714-9e1f-4270-a99b-57e47c988d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain)\n",
            "  Downloading langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.23 langchain-text-splitters-0.2.2 langsmith-0.1.93 marshmallow-3.21.3 mypy-extensions-1.0.0 orjson-3.10.6 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "GjBLaumKoenb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ni8L8hLHonI-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "-70eLnR5orAW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "id": "7BISDxbvpxg2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to upload document directly from user and keep in documents_path\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "documents_path = 'user_documents/'\n",
        "os.makedirs(documents_path, exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "  file_path = os.path.join(documents_path, filename)\n",
        "  with open(file_path, 'wb') as f:\n",
        "    f.write(uploaded[filename])\n",
        "  print(f\"Saved file {filename} to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "6QkNGlDkp0W-",
        "outputId": "61248ece-206d-41e5-a063-009127d48c21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff9a7cfb-54f6-4126-9cb5-138a6ffcaefe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff9a7cfb-54f6-4126-9cb5-138a6ffcaefe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving chapter 5.pdf to chapter 5.pdf\n",
            "Saved file chapter 5.pdf to user_documents/chapter 5.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('user_documents'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jFhSkWq4m5I",
        "outputId": "a5aafa1c-e1d4-4526-8a7d-2c4587b30191"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chapter 5.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(directory_path, glob_pattern):\n",
        "    try:\n",
        "        loader = DirectoryLoader(path=directory_path, glob=glob_pattern)\n",
        "        documents = loader.load()\n",
        "        return documents\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading documents: {e}\")\n",
        "        return []\n",
        "\n",
        "directory_path = 'user_documents/'\n",
        "glob_pattern = '**/*.pdf'\n",
        "\n",
        "documents = load_documents(directory_path, glob_pattern)\n",
        "\n",
        "if not documents:\n",
        "    print(\"No documents found.\")\n",
        "else:\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQOuxfTKxcmj",
        "outputId": "40ed5991-b45c-4d4e-9191-dd3ab2d108ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "yPeMUIDU8ZuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac05d17-34c1-4bca-f7c7-f6e9769b5980"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-1.0.8-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.7.2)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.17 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.2.23)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.17->langchain_google_genai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.17->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.17->langchain_google_genai) (0.1.93)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.17->langchain_google_genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.17->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.17->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.17->langchain_google_genai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.20.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2024.7.4)\n",
            "Downloading langchain_google_genai-1.0.8-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: langchain_google_genai\n",
            "Successfully installed langchain_google_genai-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "82f8i9ax8X-X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents[0])"
      ],
      "metadata": {
        "id": "RJ1eDhfzFSF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "VIX1e4x_Fu0z",
        "outputId": "42308902-1154-446d-8d86-0fd6e878b81d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The methods of data collection and common\\n\\nsampling techniques. Bimal Pandey\\n\\nJune 2, 2024\\n\\n1\\n\\nContents\\n\\n1 Primary and Secondary data\\n\\n2 Methods of Collecting primary data\\n\\n2.1 Observation Method . . . . . . . . . . . . . . . . . . . . . . . Interview Method . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 2.2.1 Personal Interviews . . . . . . . . . . . . . . . . . . . . 2.2.2 Telephone Interviews . . . . . . . . . . . . . . . . . . . 2.3 Collection of Data Through Questionnaires . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Collection Of Data Through Schedules\\n\\n3 Preparation of Questionnaire\\n\\n4 Types of Questions\\n\\n10 4.1 Structured Questionnaire . . . . . . . . . . . . . . . . . . . . . 10 4.2 Unstructured Questionnaire . . . . . . . . . . . . . . . . . . . 11\\n\\n5 Characteristics Of Good Questionnaire\\n\\n11 5.1 Limited Number Of Questions . . . . . . . . . . . . . . . . . . 11 5.2 Proper Sequence Of Questions . . . . . . . . . . . . . . . . . . 11 5.3 Simplicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.4 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.5 No Undesirable Questions . . . . . . . . . . . . . . . . . . . . 12 5.6 Non-controversial Question . . . . . . . . . . . . . . . . . . . . 12 5.7 Objective-type Questions . . . . . . . . . . . . . . . . . . . . . 12\\n\\n6 Concept Of Sampling\\n\\n7 Some Fundamental Definitions\\n\\n13 7.1 Population . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 7.2 Sampling Frame . . . . . . . . . . . . . . . . . . . . . . . . . . 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 7.3 Errors Sampling Error . . . . . . . . . . . . . . . . . . . . . . 14 . . . . . . . . . . . . . . . . . . . 14\\n\\n7.3.1 7.3.2 Non Sampling Error\\n\\n8 Types Of Sampling\\n\\n15 8.1 Probability Sampling . . . . . . . . . . . . . . . . . . . . . . . 16 8.1.1 Methods Of Drawing Random/Probability Sampling . 16\\n\\n2\\n\\n4\\n\\n4 5 6 7 7 8 9\\n\\n10\\n\\n12\\n\\n8.1.2 Classification Of Random/Probability Sampling . . . . 17 8.2 Non Probability Sampling . . . . . . . . . . . . . . . . . . . . 19\\n\\n9 Sample Size\\n\\n19 9.1 Sample Size For Estimating Mean . . . . . . . . . . . . . . . . 20\\n\\n3\\n\\n1 Primary and Secondary data\\n\\nData collection is the next major step in the process of conducting research that is practiced once the research problem has been identified and the re- search design/plan has been drawn out. While selecting the method of data collection to use for the study; the researcher should bear in mind two types of data; namely: primary data; and secondary data. Primary data are those data which are collected for the first time and are collected from scratch, and so these can be categorized as original.\\n\\nWhile compiled data are those that are gathered personally by the re- searcher, the secondary data are those that are already accumulated by an- other person or organization.\\n\\nelse, namely, those data that have already been through the statistical\\n\\nfilters. The researcher would have\\n\\nto finalize what sort of data he would be using (hence collecting) for the\\n\\nstudy and accordingly he will\\n\\nare faced with having to make a choice concerning which one of the two methods of data collection will suffice. The ways of obtaining primary and/or secondary data The use of primary and secondary data is one of the most common approaches to gathering information used in social research.\\n\\nSecondary data are different from primary data because while primary\\n\\ndata is to be originally collected, Secondary\\n\\nwork, the specialty of data collection work, is basically just assembly. We\\n\\ndescribe the different\\n\\ntypes of data collection techniques and the advantages and the disadvan-\\n\\ntages attached to each class of techniques.\\n\\n2 Methods of Collecting primary data\\n\\nIn anexperimental research, primary data iscollected while conducting anex- periment.\\n\\nin case we can do research of the so called descriptive type, and cary out\\n\\nsurveys, whether sample ones or population\\n\\nsurveys, then the primary data can be collected directly through obser-\\n\\nvation or through economic communication.\\n\\nto some extent through questionnaires or indirectly, with some of the\\n\\nrespondents with face-to-face interviews.\\n\\n4\\n\\nThere are several methods of collecting primary data, particularly in surveys and descriptive researches. Important ones are: (i) observation method, (ii) interview method, (iii) through questionnaires, (iv) through schedules, and (v) other methods which include (a) warranty cards; (b) distributor audits; (c) pantry audits; (d) consumer panels; (e) using mechanical devices; (f) through projective techniques; (g) depth interviews, and (h) content analysis.\\n\\n2.1 Observation Method\\n\\nThe observation method is the most widely used research technique particu- larly in regard with the behavioural sciences. We, of course, all watch things when we are conscious and living, but this sort of watching is not scientific watching. Observation becomes an appliance of scientific investigation and the accurate method of data collection for the researcher if it is for a specific purpose to solve the research problem which is well planned and documented and is also subjected to validity and reliability check. An example of this method is when the information is sought by way of actual observation by the investigator, without having to ask for it from the respondent. For instance, in a study relating to consumer behaviour, the investigator instead of asking the brand of the refrigerator possessed by a respondent, may inquire under which climate class the refrigerator is mostly used.\\n\\nby an independent and free look at the wrist watch used by the respon- dent, may himself look at the watch. The main advantage of this is that it is able to point out the extreme physical locations of any given coordinate while also providing a true user experience of the application.\\n\\nas a method of data collection it has the advantage of reducing or even eliminating the bias which may exist due to the fact that the observer has a subjective view of what he or she is observing if done objectively. Secondly, the data that one is able to collect under this method is more to do with what is actually happening at the time of data collection and not any past action pattern or future action plan/ attitude. Third, it is common to restrict this method due to its independence of the mass of respondents’\\n\\nreadiness to answer and as such can be regarded as being not extremely\\n\\ninvasive as to call for provocations from the side of\\n\\nwhat respondents as is common in the case of the interview or the ques-\\n\\ntionnaire approach. This method is\\n\\nof particular helpful when used with subjects (that is respondents) in research areas that requires interaction with human beings but such human\\n\\n5\\n\\nbeings lack the ability to comprehend and engage the research activities.\\n\\nThey are likely to provide verbal self-reports of the experiences for one\\n\\nreason or the other. However, observation method has various limitations. Firstly, it is an ex- pensive method. Secondly, the information provided by this method is very limited. Thirdly, sometimes unforeseen factors may interfere with the obser- vational task. At times, the fact that some people are rarely accessible to direct observation creates obstacle for this method to collect data effectively. While using this method, the researcher should keep in mind things like: What should be observed? How the observations should be recorded? Or how the accuracy of observation can be ensured? In case the observation is characterised by a careful definition of the units to be observed, the style of recording the observed information, standardised conditions of observation and the selection of pertinent data of observation, then the observation is called as structured observation. But when observation is to take place with- out these characteristics to be thought of in advance, the same is termed as unstructured observation. Structured observation is considered appropriate in descriptive studies, whereas in an exploratory study the observational pro- cedure is most likely to be relatively unstructured. We often talk about participant and non-participant types of observation in the context of studies, particularly of social sciences. This distinction de- pends upon the observer’s sharing or not sharing the life of the group he is observing. If the observer observes by making himself, more or less, a member of the group he is observing so that he can experience what the members of the group experience, the observation is called as the participant observation. But when the observer observes as a detached emissary without any attempt on his part to experience through participation what others feel, the observation of this type is often termed as non-participant observation. (When the observer is observing in such a manner that his presence may be unknown to the people he is observing, such an observation is described as disguised observation.)\\n\\n2.2\\n\\nInterview Method\\n\\nThe interview method of collecting data involves presentation of oral-verbal stimuli and reply in terms of oral-verbal responses. This method can be used through personal interviews and, if possible, through telephone interviews.\\n\\n6\\n\\n2.2.1 Personal Interviews\\n\\nPersonal interview method requires a person known as the interviewer asking questions, generally in a face-to-face contact to the other person or persons. (At times, the interviewee may also ask certain questions and the interviewer responds to these, but usually the interviewer initiates the interview and col- lects the information.) This sort of interview may be in the form of direct personal investigation, or it may be indirect oral investigation. In the case of direct personal investigation, the interviewer has to collect the information personally from the sources concerned. He has to be on the spot and has to meet people from whom data have to be collected. This method is partic- ularly suitable for intensive investigations. But in certain cases it may not be possible or worthwhile to contact directly the persons concerned or, on account of the extensive scope of inquiry, the direct personal investigation technique may not be used. In such cases an indirect oral examination can be conducted under which the interviewer has to cross-examine other persons who are supposed to have knowledge about the problem under investigation and the information, obtained, is recorded. Most of the commissions and committees appointed by government to carry on investigations make use of this method.\\n\\n2.2.2 Telephone Interviews\\n\\nThis method of collecting information consists in contacting respondents on telephone itself. It is not a very widely used method, but plays an important part in industrial surveys, particularly in developed regions. The chief merits of such a system are:\\n\\nIt is more flexible in comparison to mailing method.\\n\\nIt is faster than other methods, i.e., a quick way of obtaining informa- tion.\\n\\nIt is cheaper than personal interviewing method; here the cost per response is relatively low.\\n\\nRecall is easy; callbacks are simple and economical.\\n\\n7\\n\\nNo field staff is required.\\n\\nBut this system of collecting information is not free from demerits. Some of these may be highlighted.\\n\\nSurveys are restricted to respondents who have telephone facilities.\\n\\nExtensive geographical coverage may get restricted by cost considera- tions.\\n\\nIt is not suitable for intensive surveys where comprehensive answers are required to various questions.\\n\\nPossibility of the bias of the interviewer is relatively more.\\n\\nQuestions have to be short and to the point; probes are difficult to handle.\\n\\n2.3 Collection of Data Through Questionnaires\\n\\nThis method of data collection is quite popular, particularly in case of big inquiries. It is being adopted by private individuals, research workers, pri- vate and public organizations and even by governments. In this method, a questionnaire is sent (usually by post) to the persons concerned with a re- quest to answer the questions and return the questionnaire. A questionnaire consists of a number of questions printed or typed in a definite order on a form or set of forms. The questionnaire is mailed to respondents, who are expected to read and understand the questions and write down the reply in the space meant for the purpose in the questionnaire itself. The respondents have to answer the questions on their own. The method of collecting data by mailing the questionnaires to respondents is most extensively employed in various economic and business surveys. The merits claimed on behalf of this method are as follows:\\n\\nThere is low cost even when the universe is large and is widely spread geographically.\\n\\nIt is free from the bias of the interviewer; answers are in respondents’ own words.\\n\\nRespondents have adequate time to give well-thought-out answers.\\n\\n8\\n\\nRespondents, who are not easily approachable, can also be reached conveniently.\\n\\nThe main demerits of this system can also be listed here:\\n\\nLow rate of return of the duly filled in questionnaires; bias due to no-response is often indeterminate.\\n\\nIt can be used only when respondents are educated and cooperating.\\n\\nThe control over questionnaire may be lost once it is sent.\\n\\nIt is difficult to know whether willing respondents are truly represen- tative.\\n\\n2.4 Collection Of Data Through Schedules\\n\\nThis method can be compared with the method where data is collected through a format of completed questionnaires.\\n\\nlittle difference, which may be attributed by the fact that schedules entail\\n\\na pro forma containing a specified number of questions.\\n\\nwhich is duly completed with the help of the enumerators, who are made\\n\\nspecifically for the purpose. These enumerators\\n\\nIt also has schedules, return to respondents make the following questions\\n\\nfrom the pro forma in the order the\\n\\nquestions are mentioned below and the answer to the questions are written\\n\\nin the answer column provided in the pro forma. In certain\\n\\ns; circumstances, timetables may be entrusted to respondents and enu-\\n\\nmerators may assist them in entering\\n\\nthe manner they responded to each of the questions in the said schedules. Enumerators with the help of their responses provide information on the aim and objectives of\\n\\nthe investigation and also eliminate the barriers that could be encountered\\n\\nby any respondent may feel when completing the\\n\\nrelation between aspects of a given question or between the definition or\\n\\nmeaning of some challenging terms. This method of data collection is very useful in extensive inquiries and can lead to fairly reliable results. It is, however, very expensive and is usually adopted in investigations conducted by governmental agencies or by some\\n\\n9\\n\\nbig organizations. Population census all over the world is conducted through this method.\\n\\n3 Preparation of Questionnaire\\n\\nThere are nine steps involved in the development of a questionnaire:\\n\\nDecide the information required.\\n\\nDefine the target respondents.\\n\\nChoose the method(s) of reaching your target respondents.\\n\\nDecide on question content.\\n\\nDevelop the question wording.\\n\\nPut questions into a meaningful order and format.\\n\\nCheck the length of the questionnaire.\\n\\nPre-test the questionnaire.\\n\\nDevelop the final survey form.\\n\\n4 Types of Questions\\n\\nIn a broader sense, there are two types of questionnaires:\\n\\n4.1 Structured Questionnaire\\n\\nIt is also known as a closed questionnaire where such questions are asked which can be answered as yes or no. It includes less number of researchers and numerous respondents, and it has definite and concrete questions. These types of questionnaires are formal and are prepared well in advance.\\n\\n10\\n\\n4.2 Unstructured Questionnaire\\n\\nIt is based on a more open questionnaire than the previous one, which allows analyzing some parameters. An open questionnaire technically gathers more data than any other type, as the respondents are given an ability to explain in their own words and ways what is more important to them and where responses may be unlimited in length. This type of questionnaire is relatively very restrictive and can be used across many fields since they do not have to be very planned for or time-consuming.\\n\\n5 Characteristics Of Good Questionnaire\\n\\nFollowing are the qualities of good questionnaire:\\n\\n5.1 Limited Number Of Questions\\n\\nThe number of questions in the questionnaire should be as limited as possible, and questions should be asked only related to the purpose of the inquiry.\\n\\n5.2 Proper Sequence Of Questions\\n\\nQuestions must be placed in the proper sequence, like simple and direct questions must be placed at the start of the questionnaire, and hard and indirect questions must be placed at the last.\\n\\n5.3 Simplicity\\n\\nThe language of the questions should be simple and easy to understand, and the questions should be short. Complex questions must be avoided.\\n\\n5.4\\n\\nInstructions\\n\\nA good questionnaire must have clear and proper instructions for filling out the forms.\\n\\n11\\n\\n5.5 No Undesirable Questions\\n\\nUndesirable questions like personal questions, which can offend the respon- dents, must be avoided.\\n\\n5.6 Non-controversial Question\\n\\nThe question should be asked in such a way that they can be answered impartially.\\n\\n5.7 Objective-type Questions\\n\\nMore focus should be given to objective-type questions, whereas subjective- type of questions should be avoided.\\n\\n6 Concept Of Sampling\\n\\nSuppose you want to estimate the average age of the students in your class. There are two ways of doing this. The first method is to contact all students in the class, find out their ages, add them up and then divide this by the number of students (the procedure for calculating an average). The second method is to select a few students from the class, ask them their ages, add them up and then divide by the number of students you have asked. From this you can make an estimate of the average age of the class. Similarly, suppose you want to find out the average income of families living in a city. Imagine the amount of effort and resources required to go to every family in the city to find out their income! You could instead select a few families to become the basis of your enquiry and then, from what you have found out from the few families, make an estimate of the average income of families in the city. Similarly, election opinion polls can be used. These are based upon a very small group of people who are questioned about their voting preferences and, on the basis of these results, a prediction is made about the probable outcome of an election. Sampling, therefore, is the process of selecting a few (a sample) from a big- ger group (the sampling population) to become the basis for estimating or predicting the prevalence of an unknown piece of information, situation or outcome regarding the bigger group.\\n\\n12\\n\\n7 Some Fundamental Definitions\\n\\nBefore we talk about details and uses of sampling, it seems appropriate that we should be familiar with some fundamental definitions concerning sampling concepts and principles.\\n\\n7.1 Population\\n\\nFrom a statistical point of view, the term ‘Universe’refers to the total of the items or units in any field of inquiry, whereas the term ‘population’ refers to the total of items about which information is desired. The attributes that are the object of study are referred to as characteristics and the units possessing them are called as elementary units. The aggregate of such units is generally described as population. The population or universe can be finite or infinite. The population is said to be finite if it consists of a fixed number of elements so that it is possible to enumerate it in its totality. For instance, the population of a city, the number of workers in a factory are examples of finite populations. The symbol ‘N’ is generally used to indicate how many elements (or items) are there in case of a finite population. An infinite population is that population in which it is theoretically impossible to observe all the elements. Thus, in an infinite population the number of items is infinite i.e., we cannot have any idea about the total number of items. The number of stars in a sky, possible rolls of a pair of dice are examples of infinite population.\\n\\n7.2 Sampling Frame\\n\\nThe elementary units or the group or cluster of such units may form the basis of sampling process in which case they are called as sampling units. A list containing all such sampling units is known as sampling frame. Thus sampling frame consists of a list of items from which the sample is to be drawn. If the population is finite and the time frame is in the present or past, then it is possibe for the frame to be identical with the population. In most cases they are not identical because it is often impossible to draw a sample directly from population. As such this frame is either constructed by a researcher for the purpose of his study or may consist of some existing list of the population. For instance, one can use telephone directory as a\\n\\n13\\n\\nframe for conducting opinion survey in a city. Whatever the frame may be, it should be a good representative of the population.\\n\\n7.3 Errors\\n\\nStatistical error is the difference between a value obtained from a data col- lection process and the true value for the population. Data can be affected by two types of error:\\n\\n7.3.1 Sampling Error\\n\\n“Sampling error is the error that arises in a data collection process as a result of taking a sample from a population rather than using the whole population”. Even after taking care in selecting sample, there may be chances that true value is not equal to the observed value because estimation is based on the part of the population not on the whole. Hence sampling give rise to certain errors known as sampling error.\\n\\n7.3.2 Non Sampling Error\\n\\nNon-sampling error refers to all sources of error that are unrelated to sam- pling. Non-sampling errors are present in all types of survey, including cen- suses and administrative data. They arise for a number of reasons: the frame may be incomplete, some respondents may not accurately report data, data may be missing for some respondents, etc. Non-sampling errors can be classified into two groups: random errors and systematic errors.\\n\\nRandom errors are errors whose effects approximately cancel out if a large enough sample is used, leading to increased variability.\\n\\nSystematic errors are errors that tend to go in the same direction, and thus accumulate over the entire sample leading to a bias in the final results. Unlike random errors, this bias is not reduced by increasing the sample size. Systematic errors are the principal cause of concern in terms of a survey’s data quality. Unfortunately, non-sampling errors are often extremely difficult, if not impossible, to measure.\\n\\n14\\n\\n8 Types Of Sampling\\n\\nIn Statistics, there are different sampling techniques available to get relevant results from the population. The two different types of sampling methods are:\\n\\nFigure 1: Types Of Sampling\\n\\n15\\n\\n8.1 Probability Sampling\\n\\nThe probability sampling method utilizes some form of random selection. In this method, all the eligible individuals have a chance of selecting the sample from the whole sample space. This method is more time consuming and expensive than the non-probability sampling method. The benefit of using probability sampling is that it guarantees the sample that should be the representative of the population. Suppose there are 80 students in the class. Assume 20 of these refuse to participate in your study. You want the entire population of 80 students in your study but, as 20 refuse to participate, you can only use a sample of 60 students. The 20 students who refuse to participate could have strong feelings about the issues you wish to explore, but your findings will not reflect their opinions. Their exclusion from your study means that each of the 80 students does not have an equal chance of selection. Therefore, your sample does not represent the total class. There are two main advantages of random/probability sampling:\\n\\nAs they represent the total sampling population, the inferences drawn from such samples can be generalised to the total sampling population.\\n\\nSome statistical tests based upon the theory of probability can be ap- plied only to data collected from random samples.\\n\\n8.1.1 Methods Of Drawing Random/Probability Sampling\\n\\nThe fishbowl draw – if your total population is small, an easy procedure is to number each element using separate slips of paper for each element, put all the slips into a box and then pick them out one by one without looking, until the number of slips selected equals the sample size you decided upon. This method is used in some lotteries.\\n\\nComputer program – there are a number of programs that can help you to select a random sample.\\n\\nA table of randomly generated numbers – most books on research methodology and statistics include a table of randomly generated num- bers in their appendices.\\n\\n16\\n\\n8.1.2 Classification Of Random/Probability Sampling\\n\\nProbability Sampling methods are further classified into different types, such as simple random sampling, systematic sampling, stratified sampling, and clustered sampling. Let us discuss the different types of probability sampling methods along with illustrative examples here in detail.\\n\\nStratified Random Sampling- – As discussed, the accuracy of your es- timate largely depends on the extent of variability or heterogeneity of the study population with respect to the characteristics that have a strong correlation with what you are trying to ascertain (Principle 3). It follows, therefore, that if the heterogeneity in the population can be reduced by some means for a given sample size you can achieve greater accuracy in your estimate. Stratified random sampling is based upon this logic. There are two types of stratified sampling: proportionate stratified sampling and disproportionate stratified sampling. With proportion- ate stratified sampling, the number of elements from each stratum in relation to its proportion in the total population is selected, whereas in disproportionate stratified sampling, consideration is not given to the size of the stratum. For example, there are three bags (A, B and C), each with different balls. Bag A has 50 balls, bag B has 100 balls, and bag C has 200 balls. We have to choose a sample of balls from each bag proportion- ally. Suppose 5 balls from bag A, 10 balls from bag B and 20 balls from bag C.\\n\\nClustered Sampling- In the clustered sampling method, the cluster or group of people are formed from the population set. The group has similar significatory characteristics. Also, they have an equal chance of being a part of the sample. This method uses simple random sampling for the cluster of population. Imagine you want to investigate the attitude of post-secondary stu- dents in Australia towards problems in higher education in the country. Higher education institutions are in every state and territory of Aus- tralia. In addition, there are different types of institutions, for example universities, universities of technology, colleges of advanced education and colleges of technical and further education (TAFE). Within each institution various courses are offered at both undergraduate and post- 17\\n\\ngraduate levels. Each academic course could take three to four years. You can imagine the magnitude of the task. In such situations cluster sampling is extremely useful in selecting a random sample. The first level of cluster sampling could be at the state or territory level. Clusters could be grouped according to similar characteristics that ensure their comparability in terms of student population. If this is not easy, you may decide to select all the states and territories and then select a sample at the institutional level. For example, with a sim- ple random technique, one institution from each category within each state could be selected (one university, one university of technology and one TAFE college). This is based upon the assumption that institu- tions within a category are fairly similar with regards to student profile. Then, within an institution on a random basis, one or more academic programs could be selected, depending on resources. Within each study program selected, students studying in a particular year could then be selected. Further, selection of a proportion of students studying in a particular year could then be made using the SRS technique. The pro- cess of selecting a sample in this manner is called multi-stage cluster sampling.\\n\\nSystematic Sampling- Systematic sampling is a probability sampling method in which researchers select members of the population at a regular interval (or k) determined in advance. If the population order is random or random-like (e.g., alphabetical), then this method will give you a representative sample that can be used to draw conclusions about your population of interest. Let’s take an example where you want to form a sample of 500 individ- uals out of a population of 5000; you’d have to number every person in the population. Once the numbering is done, the researcher can select a number randomly, for instance, 5. The 5th individual will be the first to be a part of the systematic sample. After that, the 10th member will be added into the sample, so on and so forth (15th, 25th, 35, 45th, and members till 4995).\\n\\nMultistage Sampling- Multistage sampling is a sampling method that divides the population into groups (or clusters) for conducting research. It is a complex form of cluster sampling, sometimes, also known as multistage cluster sampling. During this sampling method, significant\\n\\n18\\n\\nclusters of the selected people are split into sub-groups at various stages to make it simpler for primary data collection. Here’s an example of a multistage design. Setting it up is easy.\\n\\nLet’s consider the sample location as the USA. The research goal is to assess the online spending trends of people in the US through an online questionnaire. Researchers can form their sample group comprising 200 households in the following manner:\\n\\nFirstly, choose the number of states using simple random sampling (or any other probability sampling). For example, select ten states. Secondly, choose five districts within each state using the systematic sampling method (or any other probability sampling). Thirdly, choose four households from each district using the systematic sampling or simple random sampling method. You will end up with 200 houses that you can include in the sample group for research.\\n\\n8.2 Non Probability Sampling\\n\\nNon-probability sampling is defined as a sampling technique in which the re- searcher selects samples based on the subjective judgment of the researcher rather than random selection. It is a less stringent method. This sampling method depends heavily on the expertise of the researchers. It is carried out by observation, and researchers use it widely for qualitative research. For example, In an organization, for studying the career goals of 500 employ- ees, technically, the sample selected should have proportionate numbers of males and females. Which means there should be 250 males and 250 females. Since this is unlikely, the researcher selects the groups or strata using quota sampling.\\n\\n9 Sample Size\\n\\nThis is the minimum sample size you need to estimate the true population proportion with the required margin of error and confidence level. Note that if some people choose not to respond they cannot be included in your sample and so if non-response is a possibility your sample size will have to be increased accordingly. In general, the higher the response rate the better the estimate, as non-response will often lead to biases in your estimate.\\n\\n19\\n\\n9.1 Sample Size For Estimating Mean\\n\\nTo determine the appropriate sample size for estimating the mean of a pop- ulation, you’ll need to consider several factors. Here’s a breakdown of the key elements involved:\\n\\nDesired Confidence Level: This represents the level of certainty you want in your estimate of the population mean. A higher confidence level means you’re more confident that the true population mean falls within a specific range based on your sample. However, it typically requires a larger sample size.\\n\\nMargin Of Error(MOE): This signifies the maximum acceptable devi- ation from the true population mean. A smaller MOE leads to a more precise estimate, but it usually necessitates a larger sample size.\\n\\nPopulation Standard Deviation: This reflects the variability of the data within the population. If you have a good estimate of the population standard deviation, it can be used to calculate a more accurate sample size. However, in many cases, the population standard deviation might be unknown.\\n\\n20'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "def split_by_page(text):\n",
        "  pages = []\n",
        "  current_page = \"\"\n",
        "\n",
        "  for line in text.splitlines():\n",
        "    if \"page\" in line.lower():\n",
        "      if current_page:\n",
        "        pages.append(current_page)\n",
        "      current_page = line + \"\\n\"\n",
        "    else:\n",
        "      current_page += line + \"\\n\"\n",
        "\n",
        "  if current_page:\n",
        "    pages.append(current_page)\n",
        "\n",
        "  if len(pages) == 1:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512, chunk_overlap = 2)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "  return pages"
      ],
      "metadata": {
        "id": "tSIO8jbTIWEz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replace 'your_api_key' with your actual API key\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Assuming `document` is a single string containing the entire document content\n",
        "pages = split_by_page(documents[0].page_content)\n",
        "print(\"pages:\",len(pages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRV4caJI8CN5",
        "outputId": "475b4c75-1238-4112-ecb1-744c3c1e05c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pages: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "MGLgrwFTO_Io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8df1f3-311e-4848-ebd6-e9956d134703"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "def get_vector_store(chunks):\n",
        "  embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "  vectorstore = FAISS.from_texts(texts=chunks, embedding = embeddings)\n",
        "  vectorstore.save_local('faiss_index')"
      ],
      "metadata": {
        "id": "MRwnY5AwE_zs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a conversational QA chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "def get_conversational_chain():\n",
        "  template = \"\"\" Answer the question as detailed as possible from the provided context, make sure to provide all details,\n",
        "  if answer is not in context do not provide\n",
        "  Context:\n",
        "  \\n{context}?\\n\n",
        "  Question: \\n{question}\\n\n",
        "  Answer :\n",
        "  \"\"\"\n",
        "\n",
        "  PROMPT = PromptTemplate(\n",
        "    template=template, input_variables=[\"context\", \"question\"]\n",
        "  )\n",
        "\n",
        "  qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\",prompt = PROMPT)\n",
        "  return qa_chain"
      ],
      "metadata": {
        "id": "YX0kz4YWAU6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the handle_user_query function\n",
        "def handle_user_query(user_query):\n",
        "    if \"call me\" in user_query.lower():\n",
        "        name = input(\"Please enter your name: \")\n",
        "        phone = input(\"Please enter your phone number: \")\n",
        "        email = input(\"Please enter your email: \")\n",
        "\n",
        "        user_info = {\n",
        "            'name': name,\n",
        "            'phone': phone,\n",
        "            'email': email\n",
        "        }\n",
        "\n",
        "        return f\"Thank you {name}. We will contact you at {phone} or {email} shortly.\", user_info\n",
        "\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "    new_db = FAISS.load_local('faiss_index', embeddings, allow_dangerous_deserialization = True)\n",
        "    docs = new_db.similarity_search(user_query)\n",
        "    chain = get_conversational_chain()\n",
        "\n",
        "\n",
        "    response = chain(\n",
        "        {\"input_documents\":docs, \"question\": user_query}\n",
        "                     ,return_only_outputs =True)\n",
        "\n",
        "    return response[\"output_text\"], None"
      ],
      "metadata": {
        "id": "GIpoJSkqASiB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chatbot function\n",
        "def chatbot():\n",
        "    get_vector_store(pages)\n",
        "    print(\"Welcome to the Document Query Chatbot!\")\n",
        "    while True:\n",
        "        user_query = input(\"You: \")\n",
        "        if \"exit\" in user_query.lower():\n",
        "          return\n",
        "        response, user_info = handle_user_query(user_query)\n",
        "        print(f\"Chatbot: {response}\")\n",
        "\n",
        "        if user_info:\n",
        "            print(\"Collected User Information:\", user_info)\n",
        "            # Here you can handle storing or using the user information as needed\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVmeKisaFAjk",
        "outputId": "94b963ab-ac87-4cc6-f310-062f6c46c92a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Document Query Chatbot!\n",
            "You: just give me basic summary of the entire pdf\n",
            "Chatbot: This document appears to be a chapter from a textbook or research paper on statistics and sampling methods. It covers various aspects of research, including:\n",
            "\n",
            "* **Fundamentals of Statistical Terms:**  Defines key terms like \"universe,\" \"population,\" \"characteristics,\" and \"elementary units.\" It also explains the concepts of finite and infinite populations.\n",
            "* **Sampling Techniques:**  Explains two main categories of sampling:\n",
            "    * **Probability Sampling:**  Describes methods like simple random sampling, systematic sampling, and stratified sampling. Provides an example of how to use these methods to select a sample of households for research.\n",
            "    * **Non-Probability Sampling:**  Mentions this category but doesn't delve into specific methods. \n",
            "* **Sample Size Determination:**  Briefly mentions the importance of determining sample size for estimating the mean of a population.\n",
            "\n",
            "The document seems to be introducing the basic concepts of sampling and lays the foundation for a more in-depth discussion of specific sampling methods and their applications. \n",
            "\n",
            "You: tell me about non-probability sampling\n",
            "Chatbot: \n",
            "You: non- probability sampling\n",
            "Chatbot: Non-probability sampling is a sampling technique where the researcher chooses samples based on their subjective judgment rather than random selection. It's a less rigorous method that relies heavily on the researcher's expertise. This method is often used for qualitative research and is carried out through observation.\n",
            "\n",
            "For example, in an organization with 500 employees, if you want to study their career goals, a non-probability sample would be selected based on the researcher's judgment about who would provide the most insightful information, rather than randomly selecting employees. \n",
            "\n",
            "You: any formulas?\n",
            "Chatbot: The provided text does not contain any formulas. \n",
            "\n",
            "You: ok thank you call me if you got it\n",
            "Please enter your name: k\n",
            "Please enter your phone number: 984399999\n",
            "Please enter your email: k.d@gmail.com\n",
            "Chatbot: Thank you k. We will contact you at 984399999 or k.d@gmail.com shortly.\n",
            "Collected User Information: {'name': 'k', 'phone': '984399999', 'email': 'k.d@gmail.com'}\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f8guyy53_eAb"
      }
    }
  ]
}